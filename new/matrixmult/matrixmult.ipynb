{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Matrix Multiplication\n",
    "\n",
    "This document introduces CUDA programming with a simple GPU square matrix\n",
    "multiplication.  The implementation used here is for demonstrating the CUDA\n",
    "parallel programming and the code is not optimized for high performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function, division\n",
    "\n",
    "# builtin packages\n",
    "import sys\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "# extra packages\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba import cuda, jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version information:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file is generated on: 2015-06-04 17:28:23.668836\n",
      "python: 3.4\n",
      "numpy: 1.9.2\n",
      "numba: 0.19.1\n",
      "CUDA GPU: b'GeForce GT 650M'\n"
     ]
    }
   ],
   "source": [
    "print(\"This file is generated on:\", datetime.datetime.now())\n",
    "print(\"python: {0}.{1}\".format(*sys.version_info[:2]))\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"numba:\", numba.__version__)\n",
    "print(\"CUDA GPU:\", cuda.gpus[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CPU Version\n",
    "\n",
    "This implements a square matrix multiplication using a naive algorithm.  We\n",
    "compile it with Numba for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def cpu_matrix_mult(matA, matB, matC):\n",
    "    m, n = matC.shape\n",
    "    k = matB.shape[0]\n",
    "    for x in range(m):\n",
    "        for y in range(n):\n",
    "            matC[x, y] = 0\n",
    "            for i in range(k):\n",
    "                matC[x, y] += matA[x, i] * matB[i, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the CPU Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create small matrices for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_dim_small = 4, 4\n",
    "matA_small = np.random.random(mat_dim_small).astype(np.float32)\n",
    "matB_small = np.random.random(mat_dim_small).astype(np.float32)\n",
    "cpu_result = np.zeros_like(matA_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpu_matrix_mult(matA_small, matB_small, cpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU result\n",
      "[[ 0.61553663  0.77477211  0.30277508  0.90226722]\n",
      " [ 1.57663691  1.10669863  1.04501224  1.87606168]\n",
      " [ 1.81146693  1.65835655  0.91621506  2.13407779]\n",
      " [ 1.05638576  0.51009035  0.96723521  1.18682384]]\n"
     ]
    }
   ],
   "source": [
    "print(\"CPU result\")\n",
    "print(cpu_result)\n",
    "assert np.allclose(np.dot(matA_small, matB_small), cpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CUDA GPU Version\n",
    "\n",
    "This implements a CUDA GPU version of the matrix multiply.\n",
    "We are using ``@cuda.jit`` to decorate the implementation to compile it into\n",
    "a *CUDA kernel*.  When the kernel function is launched, every thread\n",
    "will execute the same code.  To tell which thread the execution is in,\n",
    "CUDA provides a set of special registers that are accessible with\n",
    "``cuda.threadIdx``, ``cuda.blockIdx`` and ``cuda.blockDim``.  These registers\n",
    "are 2D or 3D vectors of the thread ID, block ID and block dimension,\n",
    "representively.\n",
    "\n",
    "CUDA defines a **thread hierarchy**.  A kernel launch creates a **grid** of\n",
    "**blocks**.  Each block contains **threads**.\n",
    "\n",
    "A common pattern is to compute the global thread ID, as oppose to\n",
    "using the nested thread and block IDs.  In this example, the kernel is launched\n",
    "with a 2D grid and 2D block that the combined dimension matches the shape of\n",
    "the matrices.  Therefore, the flattened global thread ID maps directly to the\n",
    "indices of each element in the matrix.\n",
    "\n",
    "For cases where the matrix shape is not multiple of the block dimension,\n",
    "a common practice is to launch more threads than there are elements.  The extra\n",
    "thread will have nothing to do.  It is important to check for these threads\n",
    "to avoid invalid memory reads and writes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def gpu_matrix_mult(matA, matB, matC):\n",
    "    # Read special register for thread ID\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    bw = cuda.blockDim.x\n",
    "    bh = cuda.blockDim.y\n",
    "\n",
    "    # Get global thread ID\n",
    "    x = tx + bx * bw\n",
    "    y = ty + by * bh\n",
    "\n",
    "    # Get bounds\n",
    "    m, n = matC.shape\n",
    "    k = matB.shape[0]\n",
    "\n",
    "    # Check for out-of-bound\n",
    "    if x >= m or y >= n:\n",
    "        # This is an extra thread.  Exit.\n",
    "        return\n",
    "\n",
    "    # The actual computation per output element\n",
    "    res = 0\n",
    "    for i in range(k):\n",
    "        res += matA[x, i] * matB[i, y]\n",
    "    # Store the result\n",
    "    matC[x, y] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the CUDA Code\n",
    "\n",
    "Decide of CUDA grid/block dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "block_per_grid = 60\n",
    "thread_per_block = 16\n",
    "\n",
    "griddim = block_per_grid, block_per_grid\n",
    "blockdim = thread_per_block, thread_per_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create matrices using base on the grid/block dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_dim_large = [block_per_grid * thread_per_block] * 2\n",
    "\n",
    "matA = np.random.random(mat_dim_large).astype(np.float32)\n",
    "matB = np.random.random(mat_dim_large).astype(np.float32)\n",
    "\n",
    "gpu_result = np.zeros_like(matA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch kernel\n",
    "\n",
    "The square bracket ``[]`` is overloaded to configure the launch for the grid\n",
    "and block dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpu_matrix_mult[griddim, blockdim](matA, matB, gpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 norm 0.0239563\n"
     ]
    }
   ],
   "source": [
    "npy_result = np.dot(matA, matB)\n",
    "assert np.allclose(npy_result, gpu_result)\n",
    "print(\"L1 norm\", np.linalg.norm(gpu_result - npy_result, ord=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Memory Transfers\n",
    "\n",
    "By default, numba automatically transfer numpy array memory between\n",
    "the CPU and GPU.  This is convenient but may lead to redundant memory\n",
    "transfers.  Numba will always transfer numpy array back to the CPU.\n",
    "User can control the memory transfer explicit to optimize the process.\n",
    "\n",
    "To copy to the GPU device from the CPU host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numba.cuda.cudadrv.devicearray.DeviceNDArray object at 0x111f50b38>\n",
      "<numba.cuda.cudadrv.devicearray.DeviceNDArray object at 0x111f50d30>\n"
     ]
    }
   ],
   "source": [
    "device_matA = cuda.to_device(matA)\n",
    "device_matB = cuda.to_device(matB)\n",
    "print(device_matA)\n",
    "print(device_matB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allocate GPU memory directly.  (It is similar to ``numpy.empty_like``.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numba.cuda.cudadrv.devicearray.DeviceNDArray object at 0x102e8f240>\n"
     ]
    }
   ],
   "source": [
    "device_matC = cuda.device_array_like(matA)\n",
    "print(device_matC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpu_matrix_mult[griddim, blockdim](device_matA, device_matB, device_matC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy GPU device memory back to CPU host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 242.45092773  237.57014465  235.17268372 ...,  236.98826599\n",
      "   245.49659729  238.53941345]\n",
      " [ 246.27780151  243.52182007  238.62078857 ...,  240.10771179\n",
      "   251.09939575  241.29408264]\n",
      " [ 234.05119324  231.49847412  229.52035522 ...,  229.98469543\n",
      "   241.58410645  230.72912598]\n",
      " ..., \n",
      " [ 238.38917542  230.83935547  233.30467224 ...,  231.56661987\n",
      "   246.06202698  230.62168884]\n",
      " [ 246.15298462  233.26504517  227.45283508 ...,  237.66387939\n",
      "   246.83654785  238.5874176 ]\n",
      " [ 245.53695679  240.84770203  232.91998291 ...,  234.58546448\n",
      "   241.65107727  237.06129456]]\n"
     ]
    }
   ],
   "source": [
    "gpu_result = device_matC.copy_to_host()\n",
    "print(gpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for timing function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def time_took(functor):\n",
    "    ts = timer()\n",
    "    functor()\n",
    "    te = timer()\n",
    "    return te - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for that uses ``gpu_matrix_mult()`` with manual memory transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gpu_manual_memory(matA, matB):\n",
    "    device_matC = cuda.device_array_like(matA)\n",
    "    device_matA = cuda.to_device(matA)\n",
    "    device_matB = cuda.to_device(matB)\n",
    "    gpu_matrix_mult[griddim, blockdim](device_matA, device_matB, device_matC)\n",
    "    device_matC.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   numba cpu matrix mult: 5.68 seconds\n",
      "   numba gpu matrix mult (auto transfer): 0.89 seconds\n",
      " numba gpu matrix mult (manual transfer): 0.72 seconds\n"
     ]
    }
   ],
   "source": [
    "res = np.empty_like(matA)\n",
    "cpu_time = time_took(lambda: cpu_matrix_mult(matA, matB, res))\n",
    "gpu1_time = time_took(lambda: gpu_matrix_mult[griddim, blockdim](matA, matB,\n",
    "                                                                 res))\n",
    "gpu2_time = time_took(lambda: gpu_manual_memory(matA, matB))\n",
    "\n",
    "assert gpu2_time < gpu1_time < cpu_time\n",
    "\n",
    "fmt = \"{0:>40s}: {1:.2f} seconds\"\n",
    "print(fmt.format(\"numba cpu matrix mult\", cpu_time))\n",
    "print(fmt.format(\"numba gpu matrix mult (auto transfer)\", gpu1_time))\n",
    "print(fmt.format(\"numba gpu matrix mult (manual transfer)\", gpu2_time))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}